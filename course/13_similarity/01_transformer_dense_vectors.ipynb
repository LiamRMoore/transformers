{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dense Vectors Using Transformers\n",
    "\n",
    "We will be using the [`sentence-transformers/stsb-distilbert-base`](https://huggingface.co/sentence-transformers/stsb-distilbert-base) model to build our dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize our model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6024e0d5d1147fdab2d5561abd6ac13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/539 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fb0e18cca747b78dcc5edcfb1472bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909564eda3854422bd1c706e35247cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dbb0bd8c29404d951380dd0ab8c861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c8732a5e95439da882c8cd74f725ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/489 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4558e34dd2514310a86963ac740020cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/stsb-distilbert-base')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/stsb-distilbert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tokenize a sentence just as we have been doing before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello world what a time to be alive!\"\n",
    "\n",
    "tokens = tokenizer(text, max_length=128,\n",
    "                   truncation=True, padding='max_length',\n",
    "                   return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process these tokens through our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n",
       "         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n",
       "         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n",
       "         ...,\n",
       "         [-0.7853,  0.8094, -0.2639,  ...,  0.2177,  0.3335,  0.1107],\n",
       "         [-0.7528,  0.6285, -0.0088,  ...,  0.1024,  0.4585,  0.1720],\n",
       "         [-1.0754,  0.4878, -0.3458,  ...,  0.2764,  0.5604,  0.1236]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense vector representations of our `text` are contained within the `outputs` **'last_hidden_state'** tensor, which we access like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n",
       "         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n",
       "         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n",
       "         ...,\n",
       "         [-0.7853,  0.8094, -0.2639,  ...,  0.2177,  0.3335,  0.1107],\n",
       "         [-0.7528,  0.6285, -0.0088,  ...,  0.1024,  0.4585,  0.1720],\n",
       "         [-1.0754,  0.4878, -0.3458,  ...,  0.2764,  0.5604,  0.1236]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors `embeddings`, we need to perform a *mean pooling* operation on them to create a single vector encoding (the **sentence embedding**). To do this mean pooling operation we will need to multiply each value in our `embeddings` tensor by it's respective `attention_mask` value - so that we ignore non-real tokens.\n",
    "\n",
    "To perform this operation, we first resize our `attention_mask` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's *attention_mask* status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * attention_mask.unsqueeze(-1)\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9489,  0.6905, -0.2188,  ...,  0.0161,  0.5874, -0.1449],\n",
       "         [-0.6643,  1.1984, -0.1346,  ...,  0.4839,  0.6338, -0.5003],\n",
       "         [-0.3289,  0.6412,  0.2473,  ..., -0.0965,  0.4298,  0.0515],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum the remained of the embeddings along axis `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sum the number of values that must be given attention in each position of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "         11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations `summed` divided by the number of values that should be given attention in each position `summed_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we calculate our dense similarity vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
